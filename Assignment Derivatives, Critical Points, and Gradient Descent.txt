Assignment Derivatives, Critical Points, and Gradient Descent
--------------------------------------------------------------

Problem Statement:
-------------------------
Question 1: First Order Derivative

Consider the function ğ‘“(ğ‘¥) = 3ğ‘¥^2+ 5ğ‘¥ + 2
a) Find the first-order derivative ğ‘“â€²(ğ‘¥)
b) Determine the critical points of ğ‘“(ğ‘¥) by setting ğ‘“â€²(ğ‘¥) = 0 and solving for ğ‘¥.
c) Use the first-order derivative test to classify each critical point as a local minimum, local
maximum, or neither.


Answer:
a) Find the first-order derivative ğ‘“â€²(ğ‘¥):
The given function ğ‘“(ğ‘¥) = 3ğ‘¥^2+ 5ğ‘¥ + 2
To find the first-order derivative ğ‘“â€²(ğ‘¥), we apply the power rule and sum rule: 

Applying the power rule to each term:

For a term ğ‘ğ‘¥^ğ‘›, the derivative is ğ‘›ğ‘ğ‘¥^(ğ‘›âˆ’1)

Derivative of 3ğ‘¥^2 : d/dğ‘¥(3ğ‘¥^2)= 2 x 3ğ‘¥(2-1) = 6ğ‘¥

Derivative of 5ğ‘¥   : d/dğ‘¥ = 1 x 5ğ‘¥^(1-1) = 5

Derivative of the constant term 2: 
The derivative of a constant is zero, so d/dğ‘¥(2) = 0

 Combining the derivatives using the sum rule:
ğ‘“â€²(ğ‘¥) = 6ğ‘¥+5


b) Determine the critical points of ğ‘“(ğ‘¥) by setting ğ‘“â€²(ğ‘¥) = 0 and solving for ğ‘¥
Set  6ğ‘¥+5 = 0:
6ğ‘¥ = -5
ğ‘¥ = -5/6
So the critical point is ğ‘¥ = -5/6.


c) Use the first-order derivative test to classify each critical point:
We evaluate the sign of ğ‘“â€²(ğ‘¥)around the critical point ğ‘¥ = -5/6

When ğ‘¥ < -5/6, ğ‘“â€²(ğ‘¥)< 0, indicating a local maximum.
When ğ‘¥ > -5/6, ğ‘“â€²(ğ‘¥)> 0, indicating a local minimum.

So, the critical point ğ‘¥ = âˆ’5/6 is a local maximum.
-------------------------------------------------------------------------------------------------

Question 2: Second Order Derivative
Continuing from Question 1, let ğ‘“(ğ‘¥) = 3ğ‘¥^2+ 5ğ‘¥ + 2
a) Find the second-order derivative ğ‘“â€²â€²(ğ‘¥)
b) Evaluate ğ‘“â€²â€²(ğ‘¥) at the critical points found in Question 1.
c) Use the second-order derivative test to determine whether each critical point is a local
minimum, local maximum, or neither.

Answer:
Finding the second-order derivative step by step:

The given first-order derivative is ğ‘“â€²(ğ‘¥) = 6ğ‘¥+5.

a) Apply the power rule to each term:
The power rule states that the derivative of ğ‘ğ‘¥^ğ‘› is ğ‘›ğ‘ğ‘¥^(ğ‘›âˆ’1).

Derivative of 6ğ‘¥: 
Apply the power rule with ğ‘› = 1: 
d/dğ‘¥(6ğ‘¥) = 1 x 6ğ‘¥^(1-1) = 6

Derivative of 5:
The derivative of a constant is zero, so d/dğ‘¥(5) = 0.

Combine the derivatives using the sum rule:
 ğ‘“â€²â€²(ğ‘¥) = 6

So, the second-order derivative  ğ‘“â€²â€²(ğ‘¥) is a constant, and its value is 6.


b)Evaluating ğ‘“ at the critical point found in Question 1:
The critical point found in Question 1 was ğ‘¥ = -5/6.

The given function is ğ‘“(ğ‘¥) = 3ğ‘¥^2+ 5ğ‘¥ + 2

Substitute the critical point into the function:

ğ‘“(-5/6) = 3(-5/6)^2 + 5(-5/6) + 2 

Simplify each term:
Term 1: 3(-5/6)^2
3(-5/6)^2 = 3 x 25/36 = -25/4

Term 2: 5(-5/6)
5(-5/6) = 5 x (-5/6) = -25/6

Term 3: 2
The constant term remains unchanged, so +2.

    Combine the terms:

ğ‘“(-5/6) = -25/4 - 25/6 +2

To add these fractions, find a common denominator (in this case, 12):

ğ‘“(-5/6) = -75/12 - 50/12 + 24/12

Combine the numerators:

ğ‘“(-5/6) = -101/12

So,ğ‘“(-5/6) simplifies to -101/12

c)Using the second-order derivative test to determine whether each critical point is a local
minimum, local maximum, or neither.

The second-order derivative test is a method used to determine the nature of a critical point (a point where the first derivative is equal to zero or undefined) in relation to the concavity of the function.

Given the second-order derivative  ğ‘“â€²â€²(ğ‘¥) = 6 and the critical point ğ‘¥ = -5/6.

Second-order derivative test:
If ğ‘“â€²â€²(ğ‘¥) > 0 at a critical point, then the function is concave up at that point.
If ğ‘“â€²â€²(ğ‘¥) < 0 at a critical point, then the function is concave down at that point.

In our case, ğ‘“â€²â€²(ğ‘¥) = 6, which is positive.This means that the function is concave up at the critical point ğ‘¥ = -5/6.

Implication for local extrema:

If a critical point is also a local minimum, then the function is concave up at that point.
If a critical point is also a local maximum, then the function is concave down at that point.

Since ğ‘“â€²â€²(ğ‘¥) = 6 > 0 and the function is concave up, the critical point ğ‘¥ = -5/6 is classified as a local minimum. This means that in the vicinity of ğ‘¥ = -5/6 the function ğ‘“(ğ‘¥) = 3ğ‘¥^2+ 5ğ‘¥ + 2 has a bottom point, and the graph of the function curves upward around this point.

the positive value of the second-order derivative indicates concavity up, and therefore, the critical point ğ‘¥ = -5/6 is classified as a local minimum according to the second-order derivative test.
--------------------------------------------------------------------------------------------------------------------------------------

Question 3: Chain Rule

Consider the functions ğ‘”(ğ‘¢) = ğ‘¢^3 and â„(ğ‘¥) = 2ğ‘¥ âˆ’ 1, where ğ‘¢ is a function of ğ‘¥, i.e.,
ğ‘¢ = â„(ğ‘¥).

a) Find ğ‘”â€²(ğ‘¢) and â„â€²(ğ‘¥)
b) Apply the chain rule to find ğ‘‘ğ‘”/ğ‘‘ğ‘¥


Answer:
Given functions:

ğ‘”(ğ‘¢) = ğ‘¢^3
â„(ğ‘¥) = 2ğ‘¥ âˆ’ 1
ğ‘¢ = â„(ğ‘¥)

For ğ‘”(ğ‘¢) = ğ‘¢^3
For a term ğ‘¥^ğ‘›, the derivative is ğ‘›ğ‘¥^(ğ‘›âˆ’1)
Apply the power rule: ğ‘”â€²(ğ‘¢) = 3ğ‘¢^2
or------------------------------dg/du = 3ğ‘¢^2

For â„(ğ‘¥) = 2ğ‘¥ âˆ’ 1
Apply the derivative of a linear function: â„â€²(ğ‘¥) = 2
Derivative of 2ğ‘¥   : d/dğ‘¥ = 1 x 2ğ‘¥^(1-1) = 2
Derivative of the constant term 1: 
The derivative of a constant is zero, so d/dğ‘¥(1) = 0
or ------------------------------dh/dx = â„â€²(ğ‘¥) = 2


For ğ‘¢ = â„(ğ‘¥)
---------------------------------du/dx= dh/dx


 Applying the chain rule to find ğ‘‘ğ‘”/ğ‘‘ğ‘¥

ğ‘‘ğ‘”/ğ‘‘ğ‘¥ = dg/du x du/dx
     = dg/du x du/dx
substituing the values
     = 3ğ‘¢^2 x 2
     = 2 x 3u^(2-1) x 2
     = 6u^2

as ğ‘¢ = â„(ğ‘¥)

ğ‘‘ğ‘”/ğ‘‘ğ‘¥ = 6 x (2ğ‘¥-1)^2
--------------------------------------------------------------------------------------------------------------------------------------

Question 4: Gradient Descent

A machine learning model has a cost function ğ½(Î¸) = Î¸^2 âˆ’ 4Î¸ + 5, where Î¸ is the model parameter.
a) Find the first-order partial derivative âˆ‚ğ½/âˆ‚Î¸
b) Apply the gradient descent update rule: , where is the learning rate Î¸ğ‘›ğ‘’ğ‘¤ = Î¸ğ‘œğ‘™ğ‘‘ âˆ’ Î± *âˆ‚ğ½/âˆ‚Î¸ where Î± the learning rate 
(assume Î± = 0. 1)
c) Explain how the gradient descent process helps in finding the minimum of the cost function.

Answer:
a)
Finding the first-order partial derivative of the cost function ğ½(Î¸) = Î¸^2 âˆ’ 4Î¸ + 5 with respect to Î¸ step by step:
The cost function is given by:
ğ½(Î¸) = Î¸^2 âˆ’ 4Î¸ + 5

Differentiation with respect to Î¸:
To find the first-order partial derivative, we differentiate each term of the cost function with respect to Î¸.
Differentiating Î¸^2 with respect to Î¸ gives 2Î¸.
Differentiating âˆ’4Î¸ with respect to Î¸ gives âˆ’4.
Differentiating the constant term 5 with respect to Î¸ gives 0 (as a constant has no Î¸ dependence).

So, the derivative is:
âˆ‚ğ½/âˆ‚Î¸ = 2Î¸ âˆ’4

b)
Applying the gradient descent update rule step by step:

The gradient descent update rule is given by:
Î¸ğ‘›ğ‘’ğ‘¤ = Î¸ğ‘œğ‘™ğ‘‘ âˆ’ Î± *âˆ‚ğ½/âˆ‚Î¸
Given  Î± = 0.1  substitute âˆ‚ğ½/âˆ‚Î¸ = 2Î¸ âˆ’4

Substitute into the Update Rule:
Î¸ğ‘›ğ‘’ğ‘¤ = Î¸ğ‘œğ‘™ğ‘‘ âˆ’ 0.1 (2Î¸ğ‘œğ‘™ğ‘‘âˆ’4)

Distribute the 0.1:
Î¸ğ‘›ğ‘’ğ‘¤ = Î¸ğ‘œğ‘™ğ‘‘ âˆ’ 0.1*2Î¸ğ‘œğ‘™ğ‘‘ + 0.1*4

Combine Like Terms:
Î¸ğ‘›ğ‘’ğ‘¤ = Î¸ğ‘œğ‘™ğ‘‘ - 0.2Î¸ğ‘œğ‘™ğ‘‘ +  0.4

Simplify:
Combine the terms involving Î¸ğ‘œğ‘™ğ‘‘:
Î¸ğ‘›ğ‘’ğ‘¤ = 0.8Î¸ğ‘œğ‘™ğ‘‘ + 0.4

c)
Explanation of the gradient descent process:

Gradient descent is an optimization algorithm used to minimize the cost function. It works by iteratively moving towards the minimum of the cost function by adjusting the model parameters (Î¸ in this case) in the opposite direction of the gradient.

In the given formula, âˆ‚J/âˆ‚Î¸ represents the direction and magnitude of the steepest increase of the cost function. By subtracting this gradient multiplied by the learning rate (Î±) from the current parameter value (Î¸ğ‘œğ‘™ğ‘‘), we update the parameter towards the direction of the minimum.

This process is repeated iteratively, and with each update, the model moves closer to the optimal parameter values that result in the minimum cost. The learning rate (Î±) controls the size of the steps taken in the parameter space.

In summary, gradient descent helps in finding the minimum of the cost function by iteratively adjusting the model parameters based on the negative gradient, gradually converging towards the optimal parameter values that minimize the cost.







